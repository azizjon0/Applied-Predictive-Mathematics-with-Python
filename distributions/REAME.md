## üìä Normal vs Abnormal Distributions ‚Äî Core Concepts

### üîπ What is a **Normal Distribution**?
A **bell-shaped** distribution where:

- Most values are **centered around the mean (Œº)**.
- The farther from the mean, the **less frequent** the values.
- It's **symmetrical** and smooth.
- Fully defined by two parameters:
  - **Œº (mean)** ‚Äî the center.
  - **œÉ (standard deviation)** ‚Äî the spread.

> ‚úÖ Common in real life: human height, IQ scores, measurement errors.

---

### üîπ What is an **Abnormal Distribution**?
Any distribution that **does not follow the bell curve**:

- üî∏ **Skewed** ‚Äì asymmetric (long tail to the left or right).
- üî∏ **Multimodal** ‚Äì multiple peaks.
- üî∏ **Heavy-tailed** ‚Äì outliers occur more frequently.

> ‚ö†Ô∏è These distributions often violate assumptions of standard ML models.

---

### üîπ When to Use Normal Distribution?

Use **normal distribution** when:

- ‚úÖ Data is **naturally symmetric and unimodal**.
- ‚úÖ You‚Äôre using models that assume normality:
  - Linear Regression
  - PCA
  - Gaussian Naive Bayes

Avoid using it when:

- ‚ùå Data is **skewed**, **multimodal**, or contains **outliers**.
- ‚û§ Instead, apply:
  - **Transformations** (e.g., log, Box-Cox),
  - **Robust models** (e.g., Random Forest, XGBoost).

---

### üîπ Parameters Required to Build a Distribution

You only need two:

- `Œº` ‚Äî **Mean** (location of the peak),
- `œÉ` ‚Äî **Standard deviation** (spread/width of the curve).

These two fully define the shape of a normal distribution.

---

### üîπ Where is Normal Distribution Used in ML / AI?

| Use Case                          | How It's Used                                         |
|-----------------------------------|--------------------------------------------------------|
| **Gaussian Naive Bayes**          | Assumes normal distribution for continuous features    |
| **PCA (Principal Component Analysis)** | Assumes normality for optimal component separation  |
| **Linear Regression**             | Assumes residuals follow a normal distribution         |
| **VAE (Variational Autoencoder)** | Uses Gaussian latent space for sampling                |
| **Neural Net Initialization**     | Weights often initialized from normal distribution     |
| **Gaussian Mixture Models (GMM)** | Clusters modeled as multiple normal distributions      |

---
